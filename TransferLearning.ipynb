{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9137ac-f673-457c-a7d3-c6fe2d176000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7357 images belonging to 102 classes.\n",
      "Found 1788 images belonging to 102 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 334ms/step - accuracy: 0.3633 - loss: 3.0582 - val_accuracy: 0.5464 - val_loss: 2.1011\n",
      "Epoch 2/10\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 362ms/step - accuracy: 0.5721 - loss: 1.8815 - val_accuracy: 0.6449 - val_loss: 1.5409\n",
      "Epoch 3/10\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 376ms/step - accuracy: 0.6848 - loss: 1.2980 - val_accuracy: 0.6941 - val_loss: 1.2771\n",
      "Epoch 4/10\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 430ms/step - accuracy: 0.7685 - loss: 0.9114 - val_accuracy: 0.7416 - val_loss: 1.0650\n",
      "Epoch 5/10\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 438ms/step - accuracy: 0.8289 - loss: 0.6331 - val_accuracy: 0.7489 - val_loss: 1.0432\n",
      "Epoch 6/10\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 442ms/step - accuracy: 0.8817 - loss: 0.4327 - val_accuracy: 0.7696 - val_loss: 0.9781\n",
      "Epoch 7/10\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 451ms/step - accuracy: 0.9152 - loss: 0.2940 - val_accuracy: 0.7539 - val_loss: 1.1061\n",
      "Epoch 8/10\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 454ms/step - accuracy: 0.9462 - loss: 0.1952 - val_accuracy: 0.7601 - val_loss: 1.0940\n",
      "Epoch 9/10\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 450ms/step - accuracy: 0.9600 - loss: 0.1430 - val_accuracy: 0.7578 - val_loss: 1.2141\n",
      "Epoch 10/10\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 457ms/step - accuracy: 0.9687 - loss: 0.1158 - val_accuracy: 0.7791 - val_loss: 1.1058\n",
      "Epoch 1/5\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 749ms/step - accuracy: 0.9878 - loss: 0.0465 - val_accuracy: 0.7903 - val_loss: 1.1880\n",
      "Epoch 2/5\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 905ms/step - accuracy: 0.9913 - loss: 0.0317 - val_accuracy: 0.7947 - val_loss: 1.1885\n",
      "Epoch 3/5\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 753ms/step - accuracy: 0.9940 - loss: 0.0264 - val_accuracy: 0.7959 - val_loss: 1.1728\n",
      "Epoch 4/5\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 784ms/step - accuracy: 0.9943 - loss: 0.0182 - val_accuracy: 0.7824 - val_loss: 1.2699\n",
      "Epoch 5/5\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 785ms/step - accuracy: 0.9932 - loss: 0.0253 - val_accuracy: 0.7947 - val_loss: 1.2342\n",
      "\n",
      "Validation Accuracy: 0.7947\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Transfer Learning using VGG16 (Folder-per-Class Dataset)\n",
    "# ------------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# a. Load base model with local weights (no internet required)\n",
    "base_model = VGG16(weights=None, include_top=False, input_shape=(64,64,3))\n",
    "base_model.load_weights(\"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n",
    "\n",
    "# Freeze lower layers\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# b. Add custom classifier\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(102, activation='softmax')   # Change 4 to your number of classes\n",
    "])\n",
    "\n",
    "# c. Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# d. Load dataset (single folder with subfolders per class)\n",
    "train_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_data = train_gen.flow_from_directory(\n",
    "    'caltech-101-img',             # ğŸ”¹ your dataset folder path\n",
    "    target_size=(64,64),\n",
    "    batch_size=32,\n",
    "    subset='training')\n",
    "\n",
    "val_data = train_gen.flow_from_directory(\n",
    "    'caltech-101-img',\n",
    "    target_size=(64,64),\n",
    "    batch_size=32,\n",
    "    subset='validation')\n",
    "\n",
    "# e. Train classifier\n",
    "history = model.fit(train_data, validation_data=val_data, epochs=10)\n",
    "\n",
    "# f. Fine-tune (unfreeze some deeper layers)\n",
    "for layer in base_model.layers[10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, validation_data=val_data, epochs=5)\n",
    "\n",
    "# g. Evaluate\n",
    "loss, acc = model.evaluate(val_data, verbose=0)\n",
    "print(f\"\\nValidation Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c50c7e1-3da0-4af6-a243-6ee32bfea36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4994bd10-377b-41da-8823-0bade0d4b944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959e163-251b-43c4-8d84-a1a7214af65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights =none, include_top = false, input_shape = (64,64,3))\n",
    "base_model.load_weights(\"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264a597-06bc-45f6-8cae-730353aba36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##freeze bottom layers\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = false;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff2284b-24a1-4c99-9feb-4c6f738b1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## b. custom classifier\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu')\n",
    "    Dropout(0.3)\n",
    "    Dense(128, activation='relu')\n",
    "    Dense(102, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0636c4-59b6-4462-a4cc-6eb6028c1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compile and evaluate\n",
    "model.compile (optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00871f1a-88d3-4010-8858-2be75b945e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load dataset\n",
    "train_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "train_data = train_gen.flow_from_directory(\n",
    "    'caltech-101-img',\n",
    "    target_size=(64,64),\n",
    "    batch_size=32,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data = train_gen.flow_from_directory(\n",
    "    'caltech-101-img',\n",
    "    target_size=(64,64),\n",
    "    batch_size=32,\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677f285-70aa-4bb5-ab60-1820177727a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##train model\n",
    "model.fit(train_data, validation_data= val_data, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28f2882-7195-4611-b428-c5faec7e096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f. Fine-tune (unfreeze some deeper layers)\n",
    "for layer in base_model.layers[10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, validation_data=val_data, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa61d0c-de1c-4d4f-a040-f35a81d5b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g. Evaluate\n",
    "loss, acc = model.evaluate(val_data, verbose=0)\n",
    "print(f\"\\nValidation Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1996fa-6807-41ed-946e-ee06c175ad8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2688ac4-c5d9-44c8-99e0-f8ff234d59ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
