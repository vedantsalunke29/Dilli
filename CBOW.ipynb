{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08053566-e924-4fad-a683-72b76efb64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "he speed of transmission is an important point of difference between the two viruses. Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus. The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. This means that influenza can spread faster than COVID-19. \n",
    "\n",
    "Further, transmission in the first 3-5 days of illness, or potentially pre-symptomatic transmission –transmission of the virus before the appearance of symptoms – is a major driver of transmission for influenza. In contrast, while we are learning that there are people who can shed COVID-19 virus 24-48 hours prior to symptom onset, at present, this does not appear to be a major driver of transmission. \n",
    "\n",
    "The reproductive number – the number of secondary infections generated from one infected individual – is understood to be between 2 and 2.5 for COVID-19 virus, higher than for influenza. However, estimates for both COVID-19 and influenza viruses are very context and time-specific, making direct comparisons more difficult.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d10a5-9ee9-4ac6-b4f4-b65c654218b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db0a15eb-df83-4cb5-b474-2180f4e07731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6346  \n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6303\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6269\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 4.6239\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 4.6206\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - loss: 4.6175\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6144\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6111\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6078\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6047\n",
      "\n",
      "Embedding matrix shape: (103, 8)\n",
      "\n",
      "Word: learning\n",
      "Embedding vector: [-0.02286563  0.01589054  0.02667494 -0.02162003 -0.03867946  0.04109639\n",
      " -0.00197466  0.01494199]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Lambda, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Step a: Data Preparation\n",
    "text = open('CBOW.txt', 'r').read().lower()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "word2idx = tokenizer.word_index\n",
    "vocab_size = len(word2idx) + 1\n",
    "seq = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "# Step b: Generate CBOW Training Data (context → target)\n",
    "window = 2\n",
    "contexts, targets = [], []\n",
    "for i in range(window, len(seq) - window):\n",
    "    context = [seq[i - 2], seq[i - 1], seq[i + 1], seq[i + 2]]\n",
    "    target = seq[i]\n",
    "    contexts.append(context)\n",
    "    targets.append(target)\n",
    "contexts = np.array(contexts)\n",
    "targets = np.array(targets)\n",
    "\n",
    "# Step c: Define CBOW Model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 8, input_length=4, name=\"embedding\"),  # 4 context words\n",
    "    Lambda(lambda x: tf.reduce_mean(x, axis=1)),  # average context embeddings\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.fit(contexts, targets, epochs=10, verbose=1)\n",
    "\n",
    "# Step d: Output - learned word embeddings\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "print(\"\\nEmbedding matrix shape:\", weights.shape)\n",
    "\n",
    "##Bonus\n",
    "word = 'learning'   # pick any word from your text\n",
    "if word in word2idx:\n",
    "    idx = word2idx[word]\n",
    "    print(f\"\\nWord: {word}\")\n",
    "    print(\"Embedding vector:\", weights[idx])\n",
    "else:\n",
    "    print(\"Word not found in vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8872d-1cb1-4894-9833-d73b46e979dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb1d4cb8-0478-4968-82d8-dd005113ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Lambda, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44ee608f-14fd-4e80-aaa2-357e59fb68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('CBOW.txt' , 'r').read().lower()\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "word2text = tokenizer.word_index\n",
    "vocabsize = len(word2text)+1\n",
    "seq = tokenizer.texts_to_sequences([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ec95e84-8bb1-4d6b-8620-1af551dd551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process \n",
    "window = 2\n",
    "contexts, targets = [],[]\n",
    "for i in range (window, len(seq)- window):\n",
    "    context = [[seq[i-2]], [seq[i-1]],[seq[i+1]],[seq[i+2]]]\n",
    "    target= seq[i]\n",
    "    contexts.append(context)\n",
    "    targets.append(target)\n",
    "contexts = np.array(contexts)\n",
    "targets= np.array(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13394738-26b5-4031-b6a5-667aa1d3a751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6344  \n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6300\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6264\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6230\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6194\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6159\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6124\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6090\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6053\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x131fa1c90>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step c: Define CBOW Model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 8, input_length=4, name=\"embedding\"),  # 4 context words\n",
    "    Lambda(lambda x: tf.reduce_mean(x, axis=1)),  # average context embeddings\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.fit(contexts, targets, epochs=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d629fcf4-4bb1-4a18-a241-96a0d5d2aa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding matrix shape: (103, 8)\n"
     ]
    }
   ],
   "source": [
    "##output\n",
    "weights= model.get_layer ('embedding').get_weights()[0]\n",
    "print(\"\\nEmbedding matrix shape:\", weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8c31123-4f1d-42a8-872b-5de944646231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word: 19\n"
     ]
    }
   ],
   "source": [
    "test_word = 'covid'\n",
    "\n",
    "if test_word in word2text:\n",
    "    # Create reverse mapping: index → word\n",
    "    i2w = {v: k for k, v in word2text.items()}\n",
    "\n",
    "    # Prepare the input\n",
    "    test_input = np.array([[word2text[test_word]] * 4])\n",
    "\n",
    "    # Make prediction\n",
    "    pred = model.predict(test_input, verbose=0)\n",
    "\n",
    "    # Convert predicted index to word\n",
    "    pred_word = i2w[np.argmax(pred)]\n",
    "\n",
    "    print(f\"Predicted word: {pred_word}\")\n",
    "else:\n",
    "    print(f\"'{test_word}' not found in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a12181-74b7-49b4-b632-f069cc4c082d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58abc4af-4b58-445f-bb93-3528c04b24a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7146af7-4d5f-487c-81f3-0d13836deb87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca1bca-6cbb-46ce-9062-92db93f143eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c54b9fa-5706-44ba-a37b-5737d73c6895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
